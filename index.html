<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Privacy Potions for Production ‚Äî Vector Institute</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <a href="#main" class="skip-link">Skip to content</a>
    
    <header>
        <div class="header-content">
            <div class="logos">
                <a href="https://vectorinstitute.ai" target="_blank">
                    <img src="assets/vector-logo.png" alt="Vector Institute" class="vector-logo" onerror="this.style.display='none'">
                </a>
            </div>
            <h1 class="site-title">Privacy Potions for Production</h1>
            <p class="subtitle">Controlling Leakage in AI-Synthesized Data</p>
        </div>
    </header>

    <nav class="main-nav">
        <a href="#overview">Overview</a>
        <a href="#key-findings">Key Findings</a>
        <a href="#takeaways">Industry Takeaways</a>
    </nav>

    <main id="main">
        <section id="overview" class="hero">
            <div class="container">
                <h2>Evaluating Privacy Risks in AI-Generated Synthetic Tabular Data</h2>
                
                <p class="authors">
                    <strong>Authors:</strong><br>
                    Accenture, EY, Hitachi Rail, Unilever, Vector Institute
                </p>

                <div class="cta-buttons">
                    <a href="white-paper.pdf" download class="btn btn-primary">
                        üìÑ Download White Paper
                    </a>
                </div>

                <p class="lead">
                    This white paper evaluates the privacy risks of AI-generated synthetic tabular data by analyzing the success of membership inference attacks (MIAs) under varying configurations and attacker profiles. 
                    Building on the MIDST challenge, which revealed vulnerabilities in state-of-the-art diffusion models, we identify key factors influencing privacy leakage and provide actionable guidance for practitioners.
                </p>
            </div>
        </section>

        <section id="context" class="content-section gray-bg">
            <div class="container">
                <h2>üîç The Challenge</h2>
                
                <div class="challenge-grid">
                    <div class="challenge-card">
                        <h3>‚ö†Ô∏è The Problem</h3>
                        <p>
                            Synthetic data generation promises to preserve utility while eliminating privacy risks. 
                            However, recent research reveals that generative models can leak information about specific records used during training through membership inference attacks (MIAs).
                        </p>
                    </div>
                    
                    <div class="challenge-card">
                        <h3>üéØ The MIDST Challenge</h3>
                        <p>
                            The <a href="https://vectorinstitute.github.io/MIDST/" target="_blank" rel="noopener noreferrer">Membership Inference over Diffusion-models-based Synthetic Tabular data (MIDST) challenge</a> at IEEE SaTML 2025 systematically tested privacy vulnerabilities. Over 71 participants submitted 700+ attack strategies, revealing significant privacy leakage across all scenarios.
                        </p>
                    </div>
                    
                    <div class="challenge-card">
                        <h3>üìä The Results</h3>
                        <p>
                            The winning white-box attack achieved a 46% true positive rate at a 10% false positive rate‚Äîmore than four times better than random guessing under the same metric. Even black-box attacks with only synthetic outputs reached 25% success rates, demonstrating that state-of-the-art diffusion models leak training data information.
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <section id="key-findings" class="content-section">
            <div class="container">
                <h2>üî¨ Key Experimental Findings</h2>
                
                <div class="finding">
                    <h3>1. Training Data Size Matters Most</h3>
                    <p>
                        Larger training datasets substantially improve both privacy and utility. Data collection is a more effective lever than oversynthesizing. Increasing synthetic data relative to training data degrades privacy, particularly for smaller training sets.
                    </p>
                </div>

                <div class="finding">
                    <h3>2. Model Size vs. Privacy Tradeoffs</h3>
                    <p>
                        Smaller or moderately sized models are often more privacy-efficient. Increasing model capacity can amplify privacy leakage without meaningful gains in synthetic data quality.
                    </p>
                </div>

                <div class="finding">
                    <h3>3. Hyperparameter Choices Present Clear Tradeoffs</h3>
                    <p>
                        The sensitivity of attack success to different hyperparameters is varied and nuanced. Some have large impacts while others are surprisingly unimportant. For example, increased diffusion steps and training iterations improve quality up to a point but increase vulnerability to MIAs. These hyperparameters require careful tuning based on the specific use case and acceptable risk thresholds.
                    </p>
                </div>

                <div class="finding">
                    <h3>4. MIAs Work Even with Imperfect Knowledge</h3>
                    <p>
                        MIAs remain highly effective even when attackers have imperfect knowledge of the data distribution or model parameters. This demonstrates that MIAs provide a direct, regulator-aligned measure of privacy risk.
                    </p>
                </div>

                <div class="finding">
                    <h3>5. Distance to Closest Record (DCR) Fails as a Privacy Metric</h3>
                    <p>
                        DCR and similar proxy metrics fail to reliably estimate MIA success. While DCR is easy to compute and widely used, experiments show it does not track MIA outcomes across key levers such as model size, batch size, and diffusion steps. In these scenarios, MIA success changes significantly while DCR remains largely flat. DCR cannot be relied upon as a standalone privacy metric, particularly in high-stakes or regulated settings.
                </div>
            </div>
        </section>

        <section id="takeaways" class="content-section gray-bg">
            <div class="container">
                <h2>üí° Industry Takeaways</h2>
                
                <div class="takeaway-grid">
                    <div class="takeaway-card">
                        <div class="number">1</div>
                        <h3>Prioritize Data Collection</h3>
                        <p>
                            Investing in larger, high-quality training datasets delivers better privacy and utility outcomes than attempting to compensate with excessive synthetic data generation. Conversely, generating synthetic data well in excess of your training set size, particularly with small training datasets, significantly increases privacy leakage without proportional quality gains.
                        </p>
                    </div>

                    <div class="takeaway-card">
                        <div class="number">2</div>
                        <h3>Right-Size Your Models</h3>
                        <p>
                            Don't default to the largest available model. Moderately sized models often provide the best privacy-utility balance for synthetic data generation.
                        </p>
                    </div>

                    <div class="takeaway-card">
                        <div class="number">3</div>
                        <h3>Use MIAs for Assessment</h3>
                        <p>
                            Adopt MIAs as your primary privacy risk assessment tool. They provide direct, operationally meaningful measures aligned with regulatory concerns.
                        </p>
                    </div>

                    <div class="takeaway-card">
                        <div class="number">4</div>
                        <h3>Establish Defensible Thresholds</h3>
                        <p>
                            Define acceptable privacy risk thresholds based on your specific use case, regulatory environment, and data sensitivity, even in the absence of industry-wide frameworks.
                        </p>
                    </div>

                    <div class="takeaway-card">
                        <div class="number">5</div>
                        <h3>Beware of Proxy Metrics</h3>
                        <p>
                            DCR and similar proxy metrics fail to reliably measure privacy risk. Don't rely on proxies when direct MIA-based assessment is available.
                        </p>
                    </div>

                    <div class="takeaway-card">
                        <div class="number">6</div>
                        <h3>Test Multiple Attack Scenarios</h3>
                        <p>
                            Evaluate privacy under both white-box (full model access) and black-box (synthetic outputs only) scenarios to understand your system's vulnerability profile.
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <section id="team" class="content-section gray-bg">
            <div class="container">
                <h2>üë• Participating Teams</h2>
                
                <div class="team-grid">
                    <div class="team-card">
                        <h3>Accenture</h3>
                        <p>Juan-Carlos Casta√±eda, Declan McClure, Karthik Venkataraman</p>
                    </div>
                    
                    <div class="team-card">
                        <h3>EY</h3>
                        <p>Rasoul Shahsavarifar, Jean-Luc Rukundo, N'Golo Kone, Paulina Nouwou, Yasmin Mokaberi</p>
                    </div>
                    
                    <div class="team-card">
                        <h3>Hitachi Rail</h3>
                        <p>Th√©o Pinardin, Safiya Kamal</p>
                    </div>
                    
                    <div class="team-card">
                        <h3>Unilever</h3>
                        <p>Colm Cleary, Marta Mischi</p>
                    </div>
                    
                    <div class="team-card team-card-wide">
                        <h3>Vector Institute</h3>
                        <p>
                            Masoumeh Shafieinejad & David Emerson (Technical Leads), 
                            Xi He (Faculty Advisor), Michael Joseph (Project Manager), 
                            Behnoosh Zamanlooy, Elaheh Bassak, Fatemeh Tavakoli, 
                            Sara Kodeiri, Marcelo Lotif (Research & Engineering)
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <section id="citation" class="content-section">
            <div class="container">
                <h2>üìù Citation</h2>
                
                <p>Use the BibTeX below to cite this work:</p>
                
                <div class="citation-box">
                    <pre id="bibtex-content">@techreport{vectorinstitute2026privacypotions,
  title={Privacy Potions for Production: Controlling Leakage in AI-Synthesized Data},
  author={Accenture and EY and Hitachi Rail and Unilever and {Vector Institute}},
  year={2026},
  institution={Vector Institute for Artificial Intelligence},
  type={White Paper}
}</pre>
                    <button class="copy-button" onclick="copyBibTeX()">Copy</button>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>Privacy Potions for Production</p>
            <p>White Paper ‚Ä¢ 2026</p>
        </div>
    </footer>

    <script>
        function copyBibTeX() {
            const bibtex = document.getElementById('bibtex-content').textContent;
            navigator.clipboard.writeText(bibtex).then(() => {
                alert('BibTeX citation copied to clipboard!');
            }).catch(err => {
                console.error('Failed to copy: ', err);
            });
        }

        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
    </script>
</body>
</html>
